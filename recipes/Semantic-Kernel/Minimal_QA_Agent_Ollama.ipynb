{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfbc8e3",
   "metadata": {},
   "source": [
    "# Minimal Q&A Agent Using IBM Granite via Ollama and Semantic Kernel\n",
    "\n",
    "This recipe demonstrates how to create a minimal Q&A agent using IBM Granite models through Ollama and Microsoft's Semantic Kernel framework.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Set up Ollama with IBM Granite models\n",
    "- Configure Semantic Kernel to work with Ollama\n",
    "- Create a simple Q&A agent\n",
    "- Implement chat history management\n",
    "- Build interactive conversations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Ollama**: Download and install Ollama from [ollama.ai](https://ollama.ai)\n",
    "2. **IBM Granite Model**: Pull an IBM Granite model using Ollama\n",
    "3. **Python Dependencies**: Install required Python packages\n",
    "\n",
    "### Install Ollama and Pull Granite Model\n",
    "\n",
    "First, ensure Ollama is running and pull an IBM Granite model:\n",
    "\n",
    "```bash\n",
    "# Install Ollama (macOS)\n",
    "# Download from https://ollama.ai\n",
    "\n",
    "# Pull IBM Granite 3.0 2B model\n",
    "ollama pull granite3-dense:2b\n",
    "\n",
    "# Alternative models:\n",
    "# ollama pull granite3-dense:8b\n",
    "# ollama pull granite3-moe:3b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e3b83",
   "metadata": {},
   "source": [
    "## Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Semantic Kernel and other dependencies\n",
    "!pip install semantic-kernel aiohttp python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4440c9",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56115ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.core_plugins.conversation_summary_plugin import ConversationSummaryPlugin\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deab193",
   "metadata": {},
   "source": [
    "## Configure Semantic Kernel with Ollama\n",
    "\n",
    "Set up the Semantic Kernel to use IBM Granite via Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OLLAMA_HOST = \"http://localhost:11434\"  # Default Ollama host\n",
    "MODEL_NAME = \"granite3-dense:2b\"  # IBM Granite model name\n",
    "\n",
    "# Initialize Semantic Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Ollama chat completion service\n",
    "chat_completion = OllamaChatCompletion(\n",
    "    ai_model_id=MODEL_NAME,\n",
    "    host=OLLAMA_HOST,\n",
    "    service_id=\"ollama-granite\"\n",
    ")\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "print(f\"‚úÖ Semantic Kernel configured with Ollama\")\n",
    "print(f\"üìç Host: {OLLAMA_HOST}\")\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8645a6f",
   "metadata": {},
   "source": [
    "## Create Chat History Manager\n",
    "\n",
    "Set up chat history to maintain conversation context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat history\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "# Add system message to set the agent's behavior\n",
    "chat_history.add_system_message(\n",
    "    \"You are a helpful AI assistant powered by IBM Granite. \"\n",
    "    \"You provide accurate, concise, and helpful responses to user questions. \"\n",
    "    \"If you're unsure about something, you'll say so rather than guessing.\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Chat history initialized with system prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d28489",
   "metadata": {},
   "source": [
    "## Simple Q&A Function\n",
    "\n",
    "Create a function to handle questions and maintain conversation context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4898bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_question(question: str, chat_history: ChatHistory) -> str:\n",
    "    \"\"\"\n",
    "    Ask a question to the Granite model and get a response.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question\n",
    "        chat_history (ChatHistory): The conversation history\n",
    "    \n",
    "    Returns:\n",
    "        str: The assistant's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add user message to history\n",
    "        chat_history.add_user_message(question)\n",
    "        \n",
    "        # Get chat completion service\n",
    "        chat_service = kernel.get_service(\"ollama-granite\")\n",
    "        \n",
    "        # Generate response\n",
    "        response = await chat_service.get_chat_message_content(\n",
    "            chat_history=chat_history,\n",
    "            settings=None\n",
    "        )\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        chat_history.add_assistant_message(str(response))\n",
    "        \n",
    "        return str(response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error getting response: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "print(\"‚úÖ Q&A function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd7075",
   "metadata": {},
   "source": [
    "## Test the Q&A Agent\n",
    "\n",
    "Let's test our minimal Q&A agent with some sample questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 1\n",
    "question1 = \"What is IBM Granite?\"\n",
    "print(f\"‚ùì User: {question1}\")\n",
    "\n",
    "response1 = await ask_question(question1, chat_history)\n",
    "print(f\"ü§ñ Assistant: {response1}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf65583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 2 - follow-up question to test context retention\n",
    "question2 = \"What are its main capabilities?\"\n",
    "print(f\"‚ùì User: {question2}\")\n",
    "\n",
    "response2 = await ask_question(question2, chat_history)\n",
    "print(f\"ü§ñ Assistant: {response2}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c607647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 3 - technical question\n",
    "question3 = \"How can I use Semantic Kernel with Ollama?\"\n",
    "print(f\"‚ùì User: {question3}\")\n",
    "\n",
    "response3 = await ask_question(question3, chat_history)\n",
    "print(f\"ü§ñ Assistant: {response3}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac32f24",
   "metadata": {},
   "source": [
    "## Interactive Chat Loop\n",
    "\n",
    "Create an interactive chat interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30338b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_chat():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session with the Q&A agent.\n",
    "    Type 'quit', 'exit', or 'bye' to end the conversation.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting interactive chat with IBM Granite via Ollama!\")\n",
    "    print(\"üí° Type 'quit', 'exit', or 'bye' to end the conversation.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"‚ùì You: \").strip()\n",
    "            \n",
    "            # Check for exit commands\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye', '']:\n",
    "                print(\"üëã Goodbye! Thanks for chatting!\")\n",
    "                break\n",
    "            \n",
    "            # Get and display response\n",
    "            print(\"ü§î Thinking...\")\n",
    "            response = await ask_question(user_input, chat_history)\n",
    "            print(f\"ü§ñ Assistant: {response}\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå An error occurred: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"‚úÖ Interactive chat function defined\")\n",
    "print(\"üîß Run the next cell to start an interactive chat session!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interactive chat (uncomment to run)\n",
    "# await interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935f6b9",
   "metadata": {},
   "source": [
    "## View Chat History\n",
    "\n",
    "Examine the conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat_history(chat_history: ChatHistory):\n",
    "    \"\"\"\n",
    "    Display the conversation history in a formatted way.\n",
    "    \"\"\"\n",
    "    print(\"üìú Chat History:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, message in enumerate(chat_history.messages):\n",
    "        role = message.role.value.upper()\n",
    "        content = str(message.content)[:100] + \"...\" if len(str(message.content)) > 100 else str(message.content)\n",
    "        \n",
    "        if role == \"SYSTEM\":\n",
    "            emoji = \"‚öôÔ∏è\"\n",
    "        elif role == \"USER\":\n",
    "            emoji = \"‚ùì\"\n",
    "        else:\n",
    "            emoji = \"ü§ñ\"\n",
    "            \n",
    "        print(f\"{emoji} {role}: {content}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Display current chat history\n",
    "display_chat_history(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411f0c2",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### Add Conversation Summary Plugin\n",
    "\n",
    "For longer conversations, you can use Semantic Kernel's conversation summary plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add conversation summary plugin\n",
    "kernel.add_plugin(ConversationSummaryPlugin(), plugin_name=\"conversation\")\n",
    "\n",
    "async def summarize_conversation(chat_history: ChatHistory) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the current conversation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the conversation summary function\n",
    "        summary_function = kernel.get_function(\"conversation\", \"SummarizeConversation\")\n",
    "        \n",
    "        # Convert chat history to string\n",
    "        conversation_text = \"\\n\".join([f\"{msg.role.value}: {msg.content}\" for msg in chat_history.messages])\n",
    "        \n",
    "        # Generate summary\n",
    "        result = await kernel.invoke(summary_function, input=conversation_text)\n",
    "        \n",
    "        return str(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Conversation summary plugin added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conversation summary\n",
    "if len(chat_history.messages) > 1:\n",
    "    print(\"üìù Generating conversation summary...\")\n",
    "    summary = await summarize_conversation(chat_history)\n",
    "    print(f\"üìã Summary: {summary}\")\n",
    "else:\n",
    "    print(\"üí° Not enough conversation to summarize yet. Ask a few questions first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e188420",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "### Model Parameters\n",
    "\n",
    "You can customize the model's behavior with various parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.ollama import OllamaChatPromptExecutionSettings\n",
    "\n",
    "# Create custom execution settings\n",
    "execution_settings = OllamaChatPromptExecutionSettings(\n",
    "    temperature=0.7,  # Controls randomness (0.0 = deterministic, 1.0 = very random)\n",
    "    top_p=0.9,       # Controls diversity of response\n",
    "    max_tokens=500,  # Maximum tokens in response\n",
    ")\n",
    "\n",
    "async def ask_question_with_settings(question: str, chat_history: ChatHistory, settings=None) -> str:\n",
    "    \"\"\"\n",
    "    Ask a question with custom execution settings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chat_history.add_user_message(question)\n",
    "        \n",
    "        chat_service = kernel.get_service(\"ollama-granite\")\n",
    "        \n",
    "        response = await chat_service.get_chat_message_content(\n",
    "            chat_history=chat_history,\n",
    "            settings=settings or execution_settings\n",
    "        )\n",
    "        \n",
    "        chat_history.add_assistant_message(str(response))\n",
    "        return str(response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Custom execution settings configured\")\n",
    "print(f\"üå°Ô∏è Temperature: {execution_settings.temperature}\")\n",
    "print(f\"üéØ Top-p: {execution_settings.top_p}\")\n",
    "print(f\"üìè Max tokens: {execution_settings.max_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319d1d5",
   "metadata": {},
   "source": [
    "## Test Different Models\n",
    "\n",
    "If you have multiple Granite models available, you can easily switch between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c673a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available IBM Granite models in Ollama\n",
    "available_models = [\n",
    "    \"granite3-dense:2b\",\n",
    "    \"granite3-dense:8b\", \n",
    "    \"granite3-moe:3b\",\n",
    "]\n",
    "\n",
    "def create_chat_service(model_name: str):\n",
    "    \"\"\"\n",
    "    Create a new chat service with a different model.\n",
    "    \"\"\"\n",
    "    return OllamaChatCompletion(\n",
    "        ai_model_id=model_name,\n",
    "        host=OLLAMA_HOST,\n",
    "        service_id=f\"ollama-{model_name.replace(':', '-')}\"\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Available IBM Granite models:\")\n",
    "for model in available_models:\n",
    "    print(f\"  ‚Ä¢ {model}\")\n",
    "print(\"\\nüí° You can switch models by creating a new chat service and kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785275a",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **Ollama not running**: Make sure Ollama is installed and running\n",
    "2. **Model not found**: Ensure you've pulled the IBM Granite model with `ollama pull granite3-dense:2b`\n",
    "3. **Connection issues**: Verify the Ollama host URL (default: http://localhost:11434)\n",
    "4. **Memory issues**: Try using a smaller model like `granite3-dense:2b` instead of `granite3-dense:8b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80235f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ollama connection\n",
    "import aiohttp\n",
    "import json\n",
    "\n",
    "async def test_ollama_connection():\n",
    "    \"\"\"\n",
    "    Test if Ollama is running and the model is available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # Test if Ollama is running\n",
    "            async with session.get(f\"{OLLAMA_HOST}/api/tags\") as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    models = [model['name'] for model in data.get('models', [])]\n",
    "                    \n",
    "                    print(\"‚úÖ Ollama is running\")\n",
    "                    print(f\"üì¶ Available models: {models}\")\n",
    "                    \n",
    "                    if MODEL_NAME in models:\n",
    "                        print(f\"‚úÖ Model '{MODEL_NAME}' is available\")\n",
    "                    else:\n",
    "                        print(f\"‚ùå Model '{MODEL_NAME}' not found. Please run: ollama pull {MODEL_NAME}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Ollama responded with status {response.status}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cannot connect to Ollama: {e}\")\n",
    "        print(\"üí° Make sure Ollama is installed and running\")\n",
    "\n",
    "# Run connection test\n",
    "await test_ollama_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d188a",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a working Q&A agent, you can:\n",
    "\n",
    "1. **Add more plugins**: Integrate other Semantic Kernel plugins for enhanced functionality\n",
    "2. **Create custom functions**: Build domain-specific functions for your use case\n",
    "3. **Add memory**: Implement persistent memory to remember conversations across sessions\n",
    "4. **Build a web interface**: Create a web app using frameworks like Streamlit or FastAPI\n",
    "5. **Integrate with RAG**: Combine with retrieval-augmented generation for knowledge-based responses\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Semantic Kernel Documentation](https://learn.microsoft.com/en-us/semantic-kernel/)\n",
    "- [Ollama Documentation](https://ollama.ai/docs)\n",
    "- [IBM Granite Models](https://ollama.com/blog/ibm-granite)\n",
    "- [Granite Snack Cookbook](https://github.com/ibm-granite-community/granite-snack-cookbook)\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You've successfully created a minimal Q&A agent using IBM Granite via Ollama and Semantic Kernel!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
